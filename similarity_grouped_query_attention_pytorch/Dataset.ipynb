{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN DAILY/MAIL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 257M/257M [00:07<00:00, 32.7MB/s] \n",
      "Downloading data: 100%|██████████| 257M/257M [00:06<00:00, 40.6MB/s] \n",
      "Downloading data: 100%|██████████| 259M/259M [00:07<00:00, 35.2MB/s] \n",
      "Downloading data: 100%|██████████| 34.7M/34.7M [00:01<00:00, 22.7MB/s]\n",
      "Downloading data: 100%|██████████| 30.0M/30.0M [00:00<00:00, 37.9MB/s]\n",
      "Generating train split: 100%|██████████| 287113/287113 [00:03<00:00, 81410.03 examples/s]\n",
      "Generating validation split: 100%|██████████| 13368/13368 [00:00<00:00, 91425.24 examples/s]\n",
      "Generating test split: 100%|██████████| 11490/11490 [00:00<00:00, 81273.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function_cnn_dailymail(\n",
    "        examples,\n",
    "        tokenizer,\n",
    "        max_input_length: int = config.MAX_INPUT_LENGTH,\n",
    "        max_target_length: int = config.MAX_TARGET_LENGTH,\n",
    "    ):\n",
    "        prefix = \"summarize: \"\n",
    "        inputs = [prefix + doc for doc in examples[\"article\"]]\n",
    "        model_inputs = tokenizer(\n",
    "            inputs, max_length=max_input_length, truncation=True, padding=True\n",
    "        )\n",
    "\n",
    "        # Setup the tokenizer for targets\n",
    "        labels = tokenizer(\n",
    "            text_target=examples[\"highlights\"],\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "# data_dir = \"data\"\n",
    "\n",
    "cnn_data_train = load_dataset(\n",
    "    \"cnn_dailymail\",'3.0.0', split=f\"train[:{config.PERCENT_DATA}%]\"\n",
    ")\n",
    "cnn_data_test = load_dataset(\n",
    "    \"cnn_dailymail\",'3.0.0' , split=f\"test[:{config.PERCENT_DATA}%]\"\n",
    ")\n",
    "cnn_data_val = load_dataset(\n",
    "    \"cnn_dailymail\",'3.0.0',split=f\"validation[:{config.PERCENT_DATA}%]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 287113/287113 [02:20<00:00, 2039.67 examples/s]\n",
      "Map: 100%|██████████| 13368/13368 [00:06<00:00, 2051.93 examples/s]\n",
      "Map: 100%|██████████| 11490/11490 [00:05<00:00, 2004.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, legacy=False)\n",
    "def preprocess_function_summary(examples,max_input_length:int=config.MAX_INPUT_LENGTH,max_target_length:int=config.MAX_TARGET_LENGTH):\n",
    "        prefix = \"summarize: \"\n",
    "        inputs = [prefix + doc for doc in examples[\"article\"]]\n",
    "        model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True,padding=True)\n",
    "\n",
    "        # Setup the tokenizer for targets\n",
    "        labels = tokenizer(text_target=examples[\"highlights\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "tokenized_datasets_train = cnn_data_train.map(preprocess_function_summary, batched=True,remove_columns=['article','highlights','id'],batch_size=config.TOKENIZE_BATCH_SIZE)\n",
    "tokenized_datasets_val = cnn_data_val.map(preprocess_function_summary, batched=True,remove_columns=['article','highlights','id'],batch_size=config.TOKENIZE_BATCH_SIZE)\n",
    "tokenized_datasets_test = cnn_data_test.map(preprocess_function_summary, batched=True,remove_columns=['article','highlights','id'],batch_size=config.TOKENIZE_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARXIV DATASET (TOO BIG PLANNING TO DISCARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "arxiv_link = \"https://github.com/armancohan/long-summarization/tree/master?tab=readme-ov-file\"\n",
    "# dataset = load_dataset(\"arxiv_dataset\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PubMed DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gdown\n",
    "# pubmed_url = \"https://archive.org/download/armancohan-long-summarization-paper-code/pubmed-dataset.zip\"\n",
    "# https://huggingface.co/datasets/scientific_papers?row=0\n",
    "# output = 'pubmed.zip'\n",
    "# gdown.download(pubmed_url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.txt\n",
      "vocab\n",
      "val.txt\n",
      "train.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pubmed_dir = \"pubmed-dataset\"\n",
    "\n",
    "for file in os.listdir(pubmed_dir):\n",
    "    # print(file)\n",
    "    if file.endswith(\".txt\"):\n",
    "        txt_file = os.path.join(pubmed_dir, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/athekunal/GQA/Enhancement-in-GQA/gqa-env/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for scientific_papers contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/scientific_papers\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|██████████| 5.35k/5.35k [00:00<00:00, 21.5MB/s]\n",
      "Downloading readme: 100%|██████████| 8.27k/8.27k [00:00<00:00, 16.4MB/s]\n",
      "Downloading data: 100%|██████████| 3.62G/3.62G [01:48<00:00, 33.4MB/s] \n",
      "Downloading data: 100%|██████████| 880M/880M [00:22<00:00, 39.6MB/s] \n",
      "Generating train split: 100%|██████████| 119924/119924 [00:32<00:00, 3697.11 examples/s]\n",
      "Generating validation split: 100%|██████████| 6633/6633 [00:02<00:00, 2674.18 examples/s]\n",
      "Generating test split: 100%|██████████| 6658/6658 [00:01<00:00, 5007.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "pubmed_dataset = load_dataset(\n",
    "    \"scientific_papers\",\"pubmed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'abstract', 'section_names'],\n",
       "        num_rows: 119924\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'abstract', 'section_names'],\n",
       "        num_rows: 6633\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'abstract', 'section_names'],\n",
       "        num_rows: 6658\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubmed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEDIA SUM DATASET (TOO LARGE PLANNING TO DISCARD)\n",
    "\n",
    "Link to [data](https://aclanthology.org/2021.naacl-main.474/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTI-NEWS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/athekunal/GQA/Enhancement-in-GQA/gqa-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "multi_news_dataset = load_dataset(\"multi_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 44972\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 5622\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 5622\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_news_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WMT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_en_link = \"https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.en\"\n",
    "# train_de_link = \"https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.de\"\n",
    "\n",
    "# test_en_link = \"https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/newstest2014.en\"\n",
    "# test_de_link = \"https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/newstest2014.de\"\n",
    "\n",
    "wmt_dataset = load_dataset(\"stas/wmt14-en-de-pre-processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Obama receives Netanyahu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmt_dataset['test']['translation'][0]['en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRIVIA-QA DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_type = [\"cnn_dailymail\",\"pubmed\",\"multi_news\",\"wmt14\",\"triviaqa\"]\n",
    "\n",
    "def preprocess_function(examples,dataset_name:str,max_input_length:int=config.MAX_INPUT_LENGTH,max_target_length:int=config.MAX_TARGET_LENGTH):\n",
    "        if dataset_name == \"cnn_dailymail\":\n",
    "            prefix = \"summarize: \"\n",
    "            inputs = [prefix + doc for doc in examples[\"article\"]]\n",
    "            model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True,padding=True)\n",
    "\n",
    "            # Setup the tokenizer for targets\n",
    "            labels = tokenizer(text_target=examples[\"highlights\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "        elif dataset_name == \"pubmed\":\n",
    "            prefix = \"summarize: \"\n",
    "            NotImplementedError()\n",
    "        elif dataset_name == \"multi_news\":\n",
    "            prefix = \"summarize: \"\n",
    "            inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "            model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True,padding=True)\n",
    "\n",
    "            # Setup the tokenizer for targets\n",
    "            labels = tokenizer(text_target=examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "        elif dataset_name == \"wmt14\":\n",
    "            prefix = \"translate german to english: \"\n",
    "            inputs = [prefix + doc for doc in examples[\"translation\"]['en']]\n",
    "            model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True,padding=True)\n",
    "\n",
    "            # Setup the tokenizer for targets\n",
    "            text_targets = [ex['de'] for ex in examples[\"translation\"]]\n",
    "            labels = tokenizer(text_target=text_targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "tokenized_datasets_train = cnn_data_train.map(preprocess_function, batched=True,remove_columns=['article','highlights','id'],batch_size=config.TOKENIZE_BATCH_SIZE)\n",
    "tokenized_datasets_val = cnn_data_val.map(preprocess_function, batched=True,remove_columns=['article','highlights','id'],batch_size=config.TOKENIZE_BATCH_SIZE)\n",
    "tokenized_datasets_test = cnn_data_test.map(preprocess_function, batched=True,remove_columns=['article','highlights','id'],batch_size=config.TOKENIZE_BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gqa-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
